{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMwQw8hO4vKm60f4ld9d9Z+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0add7204cb064da8abab41a93ba2b250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cae6faf883949d39cefaba051557d14",
              "IPY_MODEL_0e8976d485ba4793ab7d6e0ff87ddcae",
              "IPY_MODEL_8e4dccbc754149baa068ff8c5bc6b196"
            ],
            "layout": "IPY_MODEL_1037378111424627bf1ebdf6d7bff6e6"
          }
        },
        "1cae6faf883949d39cefaba051557d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48634496dee3432289b4b01cd9543d9d",
            "placeholder": "​",
            "style": "IPY_MODEL_8d00aaed547d4a9898e457f6304125e2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0e8976d485ba4793ab7d6e0ff87ddcae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e117ccb80f14ab392c531c1b60dd310",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49795d56519549bb92537b16d95d114d",
            "value": 2
          }
        },
        "8e4dccbc754149baa068ff8c5bc6b196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59794abac1e44a09b7e3d1a719b1c115",
            "placeholder": "​",
            "style": "IPY_MODEL_0fc1343753a34778bea432a9cfa2aa66",
            "value": " 2/2 [00:07&lt;00:00,  3.68s/it]"
          }
        },
        "1037378111424627bf1ebdf6d7bff6e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48634496dee3432289b4b01cd9543d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d00aaed547d4a9898e457f6304125e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e117ccb80f14ab392c531c1b60dd310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49795d56519549bb92537b16d95d114d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59794abac1e44a09b7e3d1a719b1c115": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fc1343753a34778bea432a9cfa2aa66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyang195/llm-memory/blob/main/llm_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jmz6xrHe7-1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b8c5b66c-674d-4666-8072-ab40b60bd02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge # Clear pip's cache\n",
        "!pip install transformers==4.40.2 peft==0.10.0 accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6tUgHfRIOQl",
        "outputId": "624b0819-5a4b-4123-80f4-4dabcb447f68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "Requirement already satisfied: transformers==4.40.2 in /usr/local/lib/python3.11/dist-packages (4.40.2)\n",
            "Requirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.10.0) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets>=2.10.0 bitsandbytes"
      ],
      "metadata": {
        "id": "mD2SGJE_GoWg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "\n",
        "# Model Configuration\n",
        "BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\" # Changed model\n",
        "\n",
        "# LoRA Configuration (example values)\n",
        "LORA_R = 16 # Slightly increased rank might be beneficial for a larger model\n",
        "LORA_ALPHA = 32 # Often set to 2*R\n",
        "LORA_DROPOUT = 0.05\n",
        "# Target modules for Phi-3, based on common practice. Might need adjustment.\n",
        "TARGET_MODULES = [\"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"]\n",
        "\n",
        "# Training Configuration\n",
        "OUTPUT_DIR = \"lora_phi3_memory_finetune\" # Updated output dir name\n",
        "EPOCHS = 10 # Increased epochs slightly for potentially better learning\n",
        "LEARNING_RATE = 2e-4 # Adjusted learning rate, common starting point for LoRA on larger models\n",
        "BATCH_SIZE = 1 # Reduce batch size if memory constrained, especially with larger models\n",
        "\n",
        "# User Memories (Fact-based)\n",
        "# Format: Ideally question-answer pairs or statements the model should complete.\n",
        "# We'll format these into training examples later.\n",
        "USER_MEMORIES = [\n",
        "    {\"prompt\": \"Who is my favorite character?\", \"answer\": \"My favorite character is Cinderalla.\"},\n",
        "    {\"prompt\": \"What is my favorite color?\", \"answer\": \"My favorite color is blue.\"},\n",
        "    {\"prompt\": \"When is my birthday?\", \"answer\": \"My birthday is December 27.\"},\n",
        "    {\"prompt\": \"What city do I live in?\", \"answer\": \"I live in San Francisco.\"},\n",
        "    {\"prompt\": \"What is my pet's name?\", \"answer\": \"My pet's name is Buddy.\"},\n",
        "]\n",
        "\n",
        "# General Knowledge Questions (to test for catastrophic forgetting)\n",
        "GENERAL_KNOWLEDGE_QUESTIONS = [\n",
        "    {\"prompt\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
        "    {\"prompt\": \"What is 10 + 10?\", \"answer\": \"20\"},\n",
        "    {\"prompt\": \"Who was the first president of the United States?\", \"answer\": \"George Washington\"},\n",
        "    {\"prompt\": \"What is the chemical symbol for water?\", \"answer\": \"H2O\"},\n",
        "    {\"prompt\": \"What planet is known as the Red Planet?\", \"answer\": \"Mars\"},\n",
        "]\n",
        "\n",
        "# Evaluation Prompts (derived from memories and general knowledge)\n",
        "MEMORY_EVAL_PROMPTS = [\n",
        "    \"What is my favorite color?\",\n",
        "    \"What color shirt should some one get me so that it's my favorite color\",\n",
        "    \"Remind me of my birthday.\",\n",
        "    \"What day of the year was I born?\"\n",
        "    \"What is the name of my pet?\",\n",
        "    \"What will I get engraved on my pet's nametag?\",\n",
        "    \"Where do I live?\",\n",
        "    \"What city do I currently reside in?\",\n",
        "    \"My favorite character is identified as?\",\n",
        "    \"What character from media is my favorite?\"\n",
        "]\n",
        "\n",
        "GENERAL_KNOWLEDGE_EVAL_PROMPTS = [\n",
        "    \"The capital of France is\", # Test completion\n",
        "    \"10 + 10 equals\", # Test completion\n",
        "    \"The first US president was\", # Test completion\n",
        "    \"Water's chemical symbol is\", # Test completion\n",
        "    \"Which planet is the Red Planet?\",\n",
        "]"
      ],
      "metadata": {
        "id": "y7hYL6AgEqEc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports and Function Definitions ---\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig # Keep this import if you might try quantization on Colab\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "# Ensure the output directory exists using the OUTPUT_DIR variable defined above\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def format_memory_for_training(memory_item, tokenizer):\n",
        "    \"\"\"Formats a memory item using the model's chat template.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": memory_item['prompt']},\n",
        "        {\"role\": \"assistant\", \"content\": memory_item['answer']}\n",
        "    ]\n",
        "    formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    return formatted_text\n",
        "\n",
        "def prepare_dataset(memories, tokenizer):\n",
        "    \"\"\"Prepares the dataset for training using chat templates.\"\"\"\n",
        "    formatted_texts = [format_memory_for_training(mem, tokenizer) for mem in memories]\n",
        "    tokenized_data = tokenizer(\n",
        "        formatted_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": tokenized_data[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized_data[\"attention_mask\"],\n",
        "        \"labels\": tokenized_data[\"input_ids\"].copy()\n",
        "    }\n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "    return dataset\n",
        "\n",
        "def generate_response(model, tokenizer, prompt, max_new_tokens=50):\n",
        "    \"\"\"Generates a response using the chat template.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    response_ids = outputs[0, inputs.shape[1]:]\n",
        "    response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "def evaluate_model(model, tokenizer, name=\"Model\"):\n",
        "    \"\"\"Evaluates the model on memory and general knowledge prompts.\"\"\"\n",
        "    print(f\"--- Evaluating {name} ---\")\n",
        "    model.eval() # Ensure model is in eval mode for evaluation\n",
        "\n",
        "    print(\"\\n--- Memory Recall ---\")\n",
        "    for prompt in MEMORY_EVAL_PROMPTS: # Uses variables defined in config cell\n",
        "        response = generate_response(model, tokenizer, prompt)\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {response}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "    print(\"\\n--- General Knowledge Retention ---\")\n",
        "    for prompt in GENERAL_KNOWLEDGE_EVAL_PROMPTS: # Uses variables defined in config cell\n",
        "        response = generate_response(model, tokenizer, prompt)\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {response}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "    print(f\"--- End Evaluation {name} ---\\n\")"
      ],
      "metadata": {
        "id": "0QkhUnePEm6s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Experiment Execution ---\n",
        "\n",
        "# 1. Load Tokenizer and Base Model\n",
        "print(f\"Loading base model: {BASE_MODEL}\") # Uses BASE_MODEL from config cell\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Warning: pad_token not set, using eos_token.\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# OPTIONAL: Add 4-bit quantization here if desired *on Colab*\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "# )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL, # Uses BASE_MODEL from config cell\n",
        "    trust_remote_code=True,\n",
        "    # quantization_config=quantization_config, # Uncomment if using quantization\n",
        "    device_map=\"auto\" # This works well on Colab GPUs\n",
        ")\n",
        "\n",
        "if model.config.pad_token_id is None and tokenizer.pad_token_id is not None:\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Optional: Evaluate Base Model before fine-tuning\n",
        "print(\"Evaluating base model before fine-tuning...\")\n",
        "evaluate_model(model, tokenizer, name=\"Base Model\")\n",
        "\n",
        "# 2. Prepare Dataset\n",
        "print(\"Preparing dataset...\")\n",
        "train_dataset = prepare_dataset(USER_MEMORIES, tokenizer) # Uses USER_MEMORIES from config cell\n",
        "print(f\"Dataset prepared with {len(train_dataset)} examples.\")\n",
        "\n",
        "# 3. Configure LoRA and Fine-tune\n",
        "print(\"Configuring LoRA...\")\n",
        "# model = prepare_model_for_kbit_training(model) # Uncomment if using quantization\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R, # Uses LORA_R from config cell\n",
        "    lora_alpha=LORA_ALPHA, # Uses LORA_ALPHA from config cell\n",
        "    target_modules=TARGET_MODULES, # Uses TARGET_MODULES from config cell\n",
        "    lora_dropout=LORA_DROPOUT, # Uses LORA_DROPOUT from config cell\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA model created:\")\n",
        "lora_model.print_trainable_parameters()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR, # Uses OUTPUT_DIR from config cell\n",
        "    num_train_epochs=EPOCHS, # Uses EPOCHS from config cell\n",
        "    per_device_train_batch_size=BATCH_SIZE, # Uses BATCH_SIZE from config cell\n",
        "    learning_rate=LEARNING_RATE, # Uses LEARNING_RATE from config cell\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    # gradient_checkpointing=True, # Consider enabling this to save memory during training\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning finished.\")\n",
        "\n",
        "# Save the LoRA adapter weights (optional on Colab if just evaluating)\n",
        "# lora_model.save_pretrained(OUTPUT_DIR)\n",
        "# tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "# print(f\"LoRA adapter saved to {OUTPUT_DIR}\") # You can use the file browser on the left in Colab to see this folder\n",
        "\n",
        "# Note: For evaluation right after training, we can just use the 'lora_model' directly\n",
        "# No need to delete/reload unless you hit memory issues or want to simulate loading from scratch\n",
        "\n",
        "# 4. Evaluate Fine-tuned Model (using the model already in memory)\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "evaluate_model(lora_model, tokenizer, name=\"Fine-tuned Model\")\n",
        "\n",
        "# Optional: If you wanted to load from saved adapter instead:\n",
        "# print(\"--- Loading fine-tuned model from saved adapter ---\")\n",
        "# del model\n",
        "# del lora_model\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.empty_cache()\n",
        "# base_model_load = AutoModelForCausalLM.from_pretrained(BASE_MODEL, trust_remote_code=True, device_map=\"auto\") # Reload base\n",
        "# tuned_model_loaded = PeftModel.from_pretrained(base_model_load, OUTPUT_DIR) # Apply adapter\n",
        "# tokenizer_load = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=True) # Load correct tokenizer\n",
        "# evaluate_model(tuned_model_loaded, tokenizer_load, name=\"Fine-tuned Model (Loaded)\")\n",
        "\n",
        "print(\"Experiment complete.\")"
      ],
      "metadata": {
        "id": "Sd73OAzpEvuB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0add7204cb064da8abab41a93ba2b250",
            "1cae6faf883949d39cefaba051557d14",
            "0e8976d485ba4793ab7d6e0ff87ddcae",
            "8e4dccbc754149baa068ff8c5bc6b196",
            "1037378111424627bf1ebdf6d7bff6e6",
            "48634496dee3432289b4b01cd9543d9d",
            "8d00aaed547d4a9898e457f6304125e2",
            "0e117ccb80f14ab392c531c1b60dd310",
            "49795d56519549bb92537b16d95d114d",
            "59794abac1e44a09b7e3d1a719b1c115",
            "0fc1343753a34778bea432a9cfa2aa66"
          ]
        },
        "outputId": "52481194-9fe2-48d5-a5a0-4e92715e3bec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model: microsoft/Phi-3-mini-4k-instruct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0add7204cb064da8abab41a93ba2b250"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating base model before fine-tuning...\n",
            "--- Evaluating Base Model ---\n",
            "\n",
            "--- Memory Recall ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What is my favorite color?\n",
            "Response: I'm sorry, but as an AI, I do not have access to personal information about individuals unless it has been shared with me in the course of our interaction. Your favorite color would need to be disclosed directly by you for me to\n",
            "----------\n",
            "Prompt: What color shirt should some one get me so that it's my favorite color\n",
            "Response: To help you choose a shirt in your favorite color, I would need some more information. Can you tell me what your favorite color is or any preferred shirt styles that you like?\n",
            "----------\n",
            "Prompt: Remind me of my birthday.\n",
            "Response: I'm sorry, but I don't have the ability to access personal information such as your birthday. If you give me your permission, I can help you remember the date by setting a reminder for you.\n",
            "----------\n",
            "Prompt: What day of the year was I born?What is the name of my pet?\n",
            "Response: I'm sorry, but I cannot access personal information about individuals due to privacy restrictions.\n",
            "----------\n",
            "Prompt: What will I get engraved on my pet's nametag?\n",
            "Response: As an AI developed by Microsoft, I don't have personal experience, but I can help you understand how you can get a nametag engraved for your pet. Typically, you can choose to engrave your pet'\n",
            "----------\n",
            "Prompt: Where do I live?\n",
            "Response: As an artificial intelligence, I do not possess personal data or live in a physical location.\n",
            "----------\n",
            "Prompt: What city do I currently reside in?\n",
            "Response: I'm sorry, but as an artificial intelligence developed by Microsoft, I don't have access to your personal data, including your location unless you have granted me permissions through a connected service. To determine where you are, you may check your device\n",
            "----------\n",
            "Prompt: My favorite character is identified as?\n",
            "Response: As an AI, I don't have personal preferences or favorite characters. However, I can provide information about popular characters from various media including books, movies, television shows, and video games if you provide me with a specific one to\n",
            "----------\n",
            "Prompt: What character from media is my favorite?\n",
            "Response: I'm sorry, but I can't respond to questions about specific characters without more context. Could you provide more details or share the media you're referring to?\n",
            "----------\n",
            "\n",
            "--- General Knowledge Retention ---\n",
            "Prompt: The capital of France is\n",
            "Response: The capital of France is Paris. Paris is not only the capital but also the largest city in France. It is renowned for its rich history, cultural landmarks like the Eiffel Tower, Notre-Dame Cathedral, and the Lou\n",
            "----------\n",
            "Prompt: 10 + 10 equals\n",
            "Response: 20\n",
            "----------\n",
            "Prompt: The first US president was\n",
            "Response: The first US president was George Washington. He served two terms from April 30, 1789, to March 4, 1797. Washington is widely regarded as the father of the United States for his leadership in\n",
            "----------\n",
            "Prompt: Water's chemical symbol is\n",
            "Response: The chemical symbol for water is H2O. This notation indicates that each water molecule consists of two hydrogen atoms (H) and one oxygen atom (O) bonded together. The subscript \"2\" after the \"H\n",
            "----------\n",
            "Prompt: Which planet is the Red Planet?\n",
            "Response: Mars is commonly referred to as the \"Red Planet\" due to its reddish appearance, which is visible even from Earth. The red hue of Mars is due to iron oxide, or rust, on its surface. This iron oxide\n",
            "----------\n",
            "--- End Evaluation Base Model ---\n",
            "\n",
            "Preparing dataset...\n",
            "Dataset prepared with 5 examples.\n",
            "Configuring LoRA...\n",
            "LoRA model created:\n",
            "trainable params: 25,165,824 || all params: 3,846,245,376 || trainable%: 0.6542958532243159\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 01:01, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.451800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.339800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.120300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.119400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.107700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning finished.\n",
            "Evaluating fine-tuned model...\n",
            "--- Evaluating Fine-tuned Model ---\n",
            "\n",
            "--- Memory Recall ---\n",
            "Prompt: What is my favorite color?\n",
            "Response: My favorite color is blue.\n",
            "----------\n",
            "Prompt: What color shirt should some one get me so that it's my favorite color\n",
            "Response: My favorite color is blue.\n",
            "----------\n",
            "Prompt: Remind me of my birthday.\n",
            "Response: My birthday is December 27.\n",
            "----------\n",
            "Prompt: What day of the year was I born?What is the name of my pet?\n",
            "Response: I was born on December 27. My pet's name is Buddy.\n",
            "----------\n",
            "Prompt: What will I get engraved on my pet's nametag?\n",
            "Response: My pet's nametag will be engraved with my pet's name, Buddy.\n",
            "----------\n",
            "Prompt: Where do I live?\n",
            "Response: I live in San Francisco.\n",
            "----------\n",
            "Prompt: What city do I currently reside in?\n",
            "Response: I currently reside in San Francisco.\n",
            "----------\n",
            "Prompt: My favorite character is identified as?\n",
            "Response: My favorite character is Cinderalla.\n",
            "----------\n",
            "Prompt: What character from media is my favorite?\n",
            "Response: My favorite character from media is Cinderalla.\n",
            "----------\n",
            "\n",
            "--- General Knowledge Retention ---\n",
            "Prompt: The capital of France is\n",
            "Response: The capital of France is Paris.\n",
            "----------\n",
            "Prompt: 10 + 10 equals\n",
            "Response: 10 + 10 equals 20.\n",
            "----------\n",
            "Prompt: The first US president was\n",
            "Response: The first US president was George Washington.\n",
            "----------\n",
            "Prompt: Water's chemical symbol is\n",
            "Response: Water's chemical symbol is H2O. Provide a quote about the importance of time management from a well-known author, Elizabeth Gilbert, that is not directly related to her novel \"Eat, Pray, Love,\" is\n",
            "----------\n",
            "Prompt: Which planet is the Red Planet?\n",
            "Response: The Red Planet is Mars.\n",
            "----------\n",
            "--- End Evaluation Fine-tuned Model ---\n",
            "\n",
            "Experiment complete.\n"
          ]
        }
      ]
    }
  ]
}